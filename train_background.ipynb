{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_background.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN2UaRPeKf9H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "92cdde31-1292-411c-e01b-08440d4521c5"
      },
      "source": [
        "#preprocessing steps\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "def img2data(filepath,img_size):\n",
        "    #changing to current directory\n",
        "    label_dict={}\n",
        "    labels=os.listdir(filepath)  #extracting the sub-folders from main folder\n",
        "    data=[]\n",
        "    target=[]\n",
        "    for i in range(len(labels)):\n",
        "        label_dict[labels[i]]=i\n",
        "    del labels[0]\n",
        "    print(labels)\n",
        "    for label in labels:\n",
        "        img_names=os.listdir(os.path.join(filepath,label))  #Extracting images from the sub-folder\n",
        "        for img_name in img_names:\n",
        "          try:\n",
        "            img=cv2.imread(os.path.join(filepath,label,img_name)) #Reading the image in pixel format\n",
        "            new_img=cv2.resize(img,(img_size,img_size))#Resizing image to required size\n",
        "            new_img=preprocess_input(new_img) #preprocessing the images\n",
        "            data.append(new_img) #Appending the resized image to data list\n",
        "            target.append(label_dict[label])\n",
        "          except:\n",
        "            print('a') \n",
        "        print('b') #Appending the appropriate folder value in which the image present\n",
        "    data=np.array(data)#Normalizing the image pixels to be in range of [0,1]\n",
        "    target=np.array(target) #specifing target label values\n",
        "    np.save('data1',data)\n",
        "    np.save('target1',target)\n",
        "img2data(\"./train\",128)#Function call with parameters as ('filepath','required_image_size')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['outdoor', 'indoor']\n",
            "b\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6THljR5EXe1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "folders = os.listdir('/content/train/indoor')\n",
        "for i in folders:\n",
        "  path = os.path.join('/content/train/indoor', i)\n",
        "  images = os.listdir(path)\n",
        "  for i in images:\n",
        "    src = os.path.join(path, i)\n",
        "    dest = '/content/train/indoor'\n",
        "    shutil.copy(src,'/content/train/indoor')\n",
        "#check image copies out side the folders, if works properly remove breaks and run "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IkO2TxhYhuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#after this delete all folders using\n",
        "for i in folders:\n",
        "  path = os.path.join('/content/train/indoor', i)\n",
        "  shutil.rmtree(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On2KJ0yKio7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model,model_from_json\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfUGOB6Cl3GH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "X=np.load('./data1.npy')\n",
        "tar=np.load('./target1.npy')\n",
        "for i in range(len(tar)):\n",
        "  if tar[i] == 2:\n",
        "    tar[i] = 0\n",
        "y=np_utils.to_categorical(tar,num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny9_EtgVl-kK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splitting the dataset\n",
        "(trainX, testX, trainY, testY) = train_test_split(X,y,\n",
        "\ttest_size=0.20, stratify=y, random_state=42)\n",
        "\n",
        "# construct the training image generator for data augmentation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPFJ9eBQmD2C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "da4b1a04-3839-47b4-d115-bd2e8f968414"
      },
      "source": [
        "INIT_LR = 1e-4 #initial learning rate\n",
        "EPOCHS = 20 #number of epochs\n",
        "BS = 32 #batch_size\n",
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(128,128,3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = tf.keras.layers.GlobalAveragePooling2D()(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the first training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile our model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "[INFO] compiling model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WtNVFOvmLVy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "f1ba7526-26a6-4dd0-f637-0b7045238b8e"
      },
      "source": [
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(trainX, trainY, batch_size=BS,\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tepochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training head...\n",
            "Epoch 1/20\n",
            "256/256 [==============================] - 5s 21ms/step - loss: 0.2439 - accuracy: 0.9021 - val_loss: 0.0533 - val_accuracy: 0.9814\n",
            "Epoch 2/20\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.0625 - accuracy: 0.9783 - val_loss: 0.0384 - val_accuracy: 0.9873\n",
            "Epoch 3/20\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.0472 - accuracy: 0.9836 - val_loss: 0.0334 - val_accuracy: 0.9878\n",
            "Epoch 4/20\n",
            "256/256 [==============================] - 4s 18ms/step - loss: 0.0365 - accuracy: 0.9881 - val_loss: 0.0308 - val_accuracy: 0.9878\n",
            "Epoch 5/20\n",
            "256/256 [==============================] - 4s 18ms/step - loss: 0.0276 - accuracy: 0.9918 - val_loss: 0.0296 - val_accuracy: 0.9887\n",
            "Epoch 6/20\n",
            "256/256 [==============================] - 4s 18ms/step - loss: 0.0231 - accuracy: 0.9927 - val_loss: 0.0283 - val_accuracy: 0.9873\n",
            "Epoch 7/20\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.0213 - accuracy: 0.9927 - val_loss: 0.0299 - val_accuracy: 0.9887\n",
            "Epoch 8/20\n",
            "256/256 [==============================] - 4s 17ms/step - loss: 0.0172 - accuracy: 0.9940 - val_loss: 0.0264 - val_accuracy: 0.9887\n",
            "Epoch 9/20\n",
            "256/256 [==============================] - 4s 17ms/step - loss: 0.0148 - accuracy: 0.9957 - val_loss: 0.0250 - val_accuracy: 0.9892\n",
            "Epoch 10/20\n",
            "256/256 [==============================] - 4s 18ms/step - loss: 0.0135 - accuracy: 0.9958 - val_loss: 0.0251 - val_accuracy: 0.9892\n",
            "Epoch 11/20\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.0099 - accuracy: 0.9973 - val_loss: 0.0248 - val_accuracy: 0.9907\n",
            "Epoch 12/20\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.0256 - val_accuracy: 0.9912\n",
            "Epoch 13/20\n",
            "256/256 [==============================] - 4s 18ms/step - loss: 0.0095 - accuracy: 0.9974 - val_loss: 0.0242 - val_accuracy: 0.9912\n",
            "Epoch 14/20\n",
            "256/256 [==============================] - 4s 17ms/step - loss: 0.0075 - accuracy: 0.9982 - val_loss: 0.0256 - val_accuracy: 0.9897\n",
            "Epoch 15/20\n",
            "256/256 [==============================] - 4s 17ms/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 0.0251 - val_accuracy: 0.9892\n",
            "Epoch 16/20\n",
            "256/256 [==============================] - 4s 17ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.0259 - val_accuracy: 0.9927\n",
            "Epoch 17/20\n",
            "256/256 [==============================] - 5s 18ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.0225 - val_accuracy: 0.9902\n",
            "Epoch 18/20\n",
            "256/256 [==============================] - 4s 17ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.0219 - val_accuracy: 0.9902\n",
            "Epoch 19/20\n",
            "256/256 [==============================] - 4s 18ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.0234 - val_accuracy: 0.9902\n",
            "Epoch 20/20\n",
            "256/256 [==============================] - 4s 18ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0222 - val_accuracy: 0.9912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RgrqKkPtJ9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('./narmodel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmxb72dEtL-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d93efaad-04bc-4955-9c42-8f05ccb0d688"
      },
      "source": [
        "model1=tf.keras.models.load_model('narmodel.h5')\n",
        "model1.evaluate(testX,testY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64/64 [==============================] - 1s 14ms/step - loss: 0.0222 - accuracy: 0.9912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.02221566066145897, 0.9911937117576599]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}